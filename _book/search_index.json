[["_main.html", "Machine Learning Performance Measurement Vorwort 1 Setup 2 Testverfahren 2.1 Cross-Validation / Kreuzvalidierung 2.2 Bootstrap sampling / Bootstrapping 3 Confusion Matrix Übersicht 4 Accuracy &amp; Error rate 5 Kappa Statistik 6 Sensitivity &amp; Specificity 7 Precision &amp; Recall 8 ROC Curves", " Machine Learning Performance Measurement Autor: Jonathan Schuster Kontakt: schuster.jonathan95@gmail.com Github: https://github.com/schuster-j 2021-12-16 Vorwort Dieses Dokument dient als Übersicht zu verschiedenen Kriterien und Kennzahlen bei der Evaluation von Machine Learning Modelle und Techniken. Der für die Beispiele genutzte Datensätze stammen aus dem Buch Machine Learning with R von Brett Lantz (Packt Publishing). Zusätzliche Inhalte wurden ergänzt. 1 Setup Das Package caret beinhaltet einige Funktionen, mit denen sehr einfach zahlreiche Machine Learning Modelle erstellt und evaluiert werden können. library(caret) Der Datensatz sms_results beinhaltet die tatsächlichen sowie von einem ML Modell hervorgesagten Klassen eines sms-Spamfilters, sowie die jeweiligen Wahrscheinlichkeiten, dass eine Nachricht einer der beiden Klassen entspricht. sms_results &lt;- read.csv(&quot;sms_results.csv&quot;, stringsAsFactors = TRUE) head(sms_results) ## actual_type predict_type prob_spam prob_ham ## 1 ham ham 0.00000 1.00000 ## 2 ham ham 0.00000 1.00000 ## 3 ham ham 0.00016 0.99984 ## 4 ham ham 0.00004 0.99996 ## 5 spam spam 1.00000 0.00000 ## 6 ham ham 0.00020 0.99980 Über die Funktion table() lässt sich eine Tabelle mit den Summen der tatsächlichen und hervorgesagten Werte erstellen: table(sms_results$actual_type, sms_results$predict_type) ## ## ham spam ## ham 1203 4 ## spam 31 152 2 Testverfahren Zur Bestimmung der Qualität eines Modells ist es unerlässlich dieses an unbekannten / zukünftigen Daten zu testen. Teilt man die vorliegenden Daten jedoch nur in jeweils einen einzelnen Trainings- und Testdatensatz, können die statistischen Kennzahlen des Modells unter Umständen zu stark davon beeinflusst werden, in welchen Proportionen die unterschiedlichen Klassen in den beiden Datensätzen vorliegen. Dies ist vor allem bei kleineren Stichproben, bzw. Stichproben, in denen eine bestimmte Klasse nur in geringer Proportion vertreten ist, von Bedeutung. Um dieses Problem zu lösen bieten sich verschiedene Verfahren an. 2.1 Cross-Validation / Kreuzvalidierung Ein mittlerweile fast schon als Standard etabliertes Testverfahren ist die sogenannte Cross-validation, bzw. k-fold corss-validation / k-fold CV. Dabei werden die Daten zufällig sortiert und in k Teilmengen, auch folds genannt, unterteilt. Anschließend werden k Modelle erstellt, bei denen jeweils eine andere dieser k Teilmengen als Testmenge verwendet wird. Jedes Modell wird dann anhand der Performance beim Vorhersagen der Testmenge evaluiert und die über alle Testmengen hinweg durchschnittliche Fehler- oder Erfolgsquote errechnet. Bei einer 10-fold CV werden somit für 10 verschiedene Modelle jeweils 90% der Daten für das Training und 10% für das Testing verwendet, wobei es zu 10 verschiedenen Kombinationen aus Trainings- und Testmengen kommt. Eine Möglichkeit die Performance eines Modells noch verlässlicher zu testen, ist die repeated k-fold CV Methode. Bei dieser wird das jeweilige k-fold CV Verfahren beliebig oft wiederholt und das Ergebnis ebenfalls über den Durchschnitt berechnet. Beispielsweise wird kann dabei ein 10-fold CV zehn mal als Ganzes wiederholt und damit mit zehn unterschiedlichen zufälligen Sortierungen der Fälle durchgeführt werden. 2.2 Bootstrap sampling / Bootstrapping Eine Alternative zum k-fold CV ist das sogenannte Bootstrap sampling / Bootstrapping. Beim Bootstrap sampling werden mehrere Trainingsdatensätze aus zufällig gewählten Fällen erstellt und die jeweils übrig gebliebenen Fälle für die Ermittlung der Performance des Modells genutzt. Der Entscheidene Unterschied zum k-fold CV ist dabei, dass ein einzelner Fall mehrfach gezogen, also im Trainingsdatensatz verwendet werden kann. Im folgenden Beispiel wurde ein Trainingsdatensatz per Bootstrapping erstellt. Der dritte und vierte Fall im Trainingsdatensatz sind identisch. Der im Trainingsdatensatz nicht vorkommende dritte Fall des originalen Datensatzes (Happy, Yes, 80, Pop) würde in diesem Fall beim Testen des Modells verwendet werden. 3 Confusion Matrix Übersicht Die confusion matrix, die durch den Befehl confusionMatrix() aus dem caret Package erstellt werden kann, ermöglicht nicht nur den Vergleich der tatsächlichen und hervorgesagten Werte, sondern berechnet zusätzlich Informationen und Kennzahlen über die Qualität der Vorhersagen. So lassen sich im Output unter anderem Werte für Accuracy, Kappa, Sensitivity, Specificity finden, die im Folgenden genauer erläutert werden. confusionMatrix(sms_results$predict_type, sms_results$actual_type, positive = &quot;spam&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction ham spam ## ham 1203 31 ## spam 4 152 ## ## Accuracy : 0.9748 ## 95% CI : (0.9652, 0.9824) ## No Information Rate : 0.8683 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 0.8825 ## ## Mcnemar&#39;s Test P-Value : 1.109e-05 ## ## Sensitivity : 0.8306 ## Specificity : 0.9967 ## Pos Pred Value : 0.9744 ## Neg Pred Value : 0.9749 ## Prevalence : 0.1317 ## Detection Rate : 0.1094 ## Detection Prevalence : 0.1122 ## Balanced Accuracy : 0.9136 ## ## &#39;Positive&#39; Class : spam ## 4 Accuracy &amp; Error rate Accuracy oder auch success rate definiert den Anteil der korrekt hervorgesagten positiven und negativen Werte: Die Anzahl der einzelnen Fälle je nach Klasse können aus der bereits erstellten Tabelle entnommen werden: table(sms_results$actual_type, sms_results$predict_type) ## ## ham spam ## ham 1203 4 ## spam 31 152 accuracy &lt;- (152 + 1203) / (152 + 1203 + 4 + 31) print(accuracy) ## [1] 0.9748201 Die Fehlerrate / error rate ergibt sich folglich aus dem Anteil der falsch hervorgesagten Werte oder der Differenz von 1 und der accuracy: error_rate &lt;- (4 + 31) / (152 + 1203 + 4 + 31) print(error_rate) ## [1] 0.02517986 5 Kappa Statistik Die Kappa Statistik passt die accuracy unter Berücksichtigung der Wahrscheinlichkeit einer korrekten Hervorsage durch Zufall an. Sie ist daher vor allem bei Datensätzen, in denen die entsprechende Klasse ungleich verteilt ist von Bedeutung. Pr(a) bezieht sich auf die Proportion von Übereinstimmungen der tatsächlichen und hervorgesagten Werte. Pr(e) bezieht sich auf die erwartete Übereinstimmung der Werte unter Annahme, dass diese zufällig gewählt worden sind: Genau wie die accuracy kann die Kappa Statistik aus den in der Kreuztabelle der tatsächlichen und hervorgesagten Werten berechnet werden und wird in dem Output der Funktion confusionMatrix() aus dem Package caret angezeigt. Alternativ lässt sich die Kappa mit der Funktion Kappa() aus dem Package vcd berechnen. library(vcd) Kappa(table(sms_results$actual_type, sms_results$predict_type)) ## value ASE z Pr(&gt;|z|) ## Unweighted 0.8825 0.01949 45.27 0 ## Weighted 0.8825 0.01949 45.27 0 6 Sensitivity &amp; Specificity Sensitivity (oder auch true positive rate) misst die Proportion der positiven Fälle, die korrekt klassifiziert wurden: Neben der manuellen Berechnung anhand der Werte aus der Kreuztabelle, kann die sensitivity durch die Funktion sensitivity() aus dem Package caret berechnet werden: sensitivity(sms_results$predict_type, sms_results$actual_type, positive = &quot;spam&quot;) ## [1] 0.8306011 Specificity (oder auch true negative rate) misst hingegen die Proportion der negative Fälle, die korrekt klassifiziert wurden: Die specificity kann ebenfalls mit Hilfe von caret berechnet werden: specificity(sms_results$predict_type, sms_results$actual_type, negative = &quot;ham&quot;) ## [1] 0.996686 7 Precision &amp; Recall Precision (oder auch positive predictive value) misst die Proportion der positiven hervorgesagten Fälle, die tatsächlich positiv sind. In anderen Worten: Wieviele der positiv klassifizierten Fälle sind korrekt? posPredValue(sms_results$predict_type, sms_results$actual_type, positive = &quot;spam&quot;) ## [1] 0.974359 Recall misst die Vollständigkeit der Ergebnisse und entspricht der sensitivity. sensitivity(sms_results$predict_type, sms_results$actual_type, positive = &quot;spam&quot;) ## [1] 0.8306011 8 ROC Curves Die Receiver Operating Characteristic (ROC) Curve wird dazu genutzt den Kompromiss zwischen dem Entdecken von true positives und dem Vermeiden von false positives zu prüfen. Dabei wird der Anteil der true positives auf einer vertikalen und der Anteil der false positives auf einer horizontalen Achse dargestellt. Eine diagonale würde dabei einen Classifier repräsentieren, der keinen hervorsagenden Wert besitzt und true positives sowie false positives mit der exakt gleichen Rate hervorsagt. Ein perfekter Classifier würde hingegen durch den Punkt von 100% true positive rate und 0% false positive rate gehen. library(pROC) sms_roc &lt;- roc(predictor = sms_results$prob_spam, sms_results$actual_type) roc_plot &lt;- plot(sms_roc, main = &quot;ROC curve for SMS spam filter&quot;, col = &quot;blue&quot;, lwd = 2, legacy.axes = TRUE) Je näher die ROC Kurve dem perfekten Classifier ist, umso besser ist die Hervorsage positiver Fälle. Um dies zu messen, wird die Fläche unterhalb der ROC Kurve, die sogenannte area under the ROC curve (AUC) berechnet. auc(sms_roc) ## Area under the curve: 0.9836 "]]
